{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL with PySpark and Airflow Demo Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to be a companion to the demo video. You can watch the video and follow along yourself, go through this notebook without watching the video, or watch and use the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm location of java home and spark home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/opt/openjdk@22\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.1/libexec\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/spark-submit\n"
     ]
    }
   ],
   "source": [
    "# Find JAVA_HOME\n",
    "import os\n",
    "print(os.environ['JAVA_HOME'])\n",
    "\n",
    "# Find SPARK_HOME\n",
    "print(os.environ['SPARK_HOME'])\n",
    "\n",
    "# Confirm path to spark-submit\n",
    "print(os.environ['SPARK_HOME'] + '/bin/spark-submit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize virtual environment and install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should already be done before you start the notebook, but if you haven't, make and activate a virtual environment now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
      "Collecting apache-airflow\n",
      "  Using cached apache_airflow-2.9.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting dag-factory\n",
      "  Using cached dag_factory-0.19.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pytest\n",
      "  Using cached pytest-8.3.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting alembic<2.0,>=1.13.1 (from apache-airflow)\n",
      "  Using cached alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting argcomplete>=1.10 (from apache-airflow)\n",
      "  Using cached argcomplete-3.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting asgiref (from apache-airflow)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting attrs>=22.1.0 (from apache-airflow)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker>=1.6.2 (from apache-airflow)\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting colorlog<5.0,>=4.0.2 (from apache-airflow)\n",
      "  Using cached colorlog-4.8.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting configupdater>=3.1.1 (from apache-airflow)\n",
      "  Using cached ConfigUpdater-3.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting connexion<3.0,>=2.10.0 (from connexion[flask]<3.0,>=2.10.0->apache-airflow)\n",
      "  Using cached connexion-2.14.2-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting cron-descriptor>=1.2.24 (from apache-airflow)\n",
      "  Using cached cron_descriptor-1.4.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting croniter>=2.0.2 (from apache-airflow)\n",
      "  Using cached croniter-3.0.3-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting cryptography>=39.0.0 (from apache-airflow)\n",
      "  Using cached cryptography-43.0.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.4 kB)\n",
      "Collecting deprecated>=1.2.13 (from apache-airflow)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dill>=0.2.2 (from apache-airflow)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting flask-caching>=1.5.0 (from apache-airflow)\n",
      "  Using cached Flask_Caching-2.3.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting flask-session<0.6,>=0.4.0 (from apache-airflow)\n",
      "  Using cached flask_session-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting flask-wtf>=0.15 (from apache-airflow)\n",
      "  Using cached flask_wtf-1.2.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting flask<2.3,>=2.2 (from apache-airflow)\n",
      "  Using cached Flask-2.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting fsspec>=2023.10.0 (from apache-airflow)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-re2>=1.0 (from apache-airflow)\n",
      "  Using cached google_re2-1.1.20240702-1-cp312-cp312-macosx_14_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting gunicorn>=20.1.0 (from apache-airflow)\n",
      "  Using cached gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting httpx (from apache-airflow)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting itsdangerous>=2.0 (from apache-airflow)\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting jinja2>=3.0.0 (from apache-airflow)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema>=4.18.0 (from apache-airflow)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting lazy-object-proxy (from apache-airflow)\n",
      "  Using cached lazy_object_proxy-1.10.0-cp312-cp312-macosx_14_0_arm64.whl\n",
      "Collecting linkify-it-py>=2.0.0 (from apache-airflow)\n",
      "  Using cached linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting lockfile>=0.12.2 (from apache-airflow)\n",
      "  Using cached lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.1.0 (from apache-airflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting markupsafe>=1.1.1 (from apache-airflow)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting marshmallow-oneofschema>=2.0.1 (from apache-airflow)\n",
      "  Using cached marshmallow_oneofschema-3.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting mdit-py-plugins>=0.3.0 (from apache-airflow)\n",
      "  Using cached mdit_py_plugins-0.4.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting methodtools>=0.4.7 (from apache-airflow)\n",
      "  Using cached methodtools-0.4.7-py3-none-any.whl\n",
      "Collecting opentelemetry-api>=1.15.0 (from apache-airflow)\n",
      "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp (from apache-airflow)\n",
      "  Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: packaging>=14.0 in ./.venv/lib/python3.12/site-packages (from apache-airflow) (24.1)\n",
      "Collecting pathspec>=0.9.0 (from apache-airflow)\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pendulum<4.0,>=2.1.2 (from apache-airflow)\n",
      "  Using cached pendulum-3.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
      "Collecting pluggy>=1.0 (from apache-airflow)\n",
      "  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: psutil>=4.2.0 in ./.venv/lib/python3.12/site-packages (from apache-airflow) (6.0.0)\n",
      "Requirement already satisfied: pygments>=2.0.1 in ./.venv/lib/python3.12/site-packages (from apache-airflow) (2.18.0)\n",
      "Collecting pyjwt>=2.0.0 (from apache-airflow)\n",
      "  Using cached PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting python-daemon>=3.0.0 (from apache-airflow)\n",
      "  Using cached python_daemon-3.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.3 in ./.venv/lib/python3.12/site-packages (from apache-airflow) (2.9.0.post0)\n",
      "Collecting python-nvd3>=0.15.0 (from apache-airflow)\n",
      "  Using cached python_nvd3-0.16.0-py3-none-any.whl\n",
      "Collecting python-slugify>=5.0 (from apache-airflow)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting requests<3,>=2.27.0 (from apache-airflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rfc3339-validator>=0.1.4 (from apache-airflow)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rich-argparse>=1.0.0 (from apache-airflow)\n",
      "  Using cached rich_argparse-1.5.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting rich>=12.4.4 (from apache-airflow)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting setproctitle>=1.1.8 (from apache-airflow)\n",
      "  Using cached setproctitle-1.3.3-cp312-cp312-macosx_10_9_universal2.whl.metadata (9.9 kB)\n",
      "Collecting sqlalchemy<2.0,>=1.4.36 (from apache-airflow)\n",
      "  Using cached SQLAlchemy-1.4.53-cp312-cp312-macosx_10_9_universal2.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow)\n",
      "  Using cached SQLAlchemy_JSONField-1.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tabulate>=0.7.5 (from apache-airflow)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tenacity!=8.2.0,>=6.2.0 (from apache-airflow)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting termcolor>=1.1.0 (from apache-airflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting unicodecsv>=0.14.1 (from apache-airflow)\n",
      "  Using cached unicodecsv-0.14.1-py3-none-any.whl\n",
      "Collecting universal-pathlib>=0.2.2 (from apache-airflow)\n",
      "  Using cached universal_pathlib-0.2.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting werkzeug<3,>=2.0 (from apache-airflow)\n",
      "  Using cached werkzeug-2.3.8-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting apache-airflow-providers-common-io (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_common_io-1.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting apache-airflow-providers-common-sql (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_common_sql-1.15.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting apache-airflow-providers-fab>=1.0.2 (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_fab-1.2.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting apache-airflow-providers-ftp (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_ftp-3.10.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting apache-airflow-providers-http (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_http-4.12.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting apache-airflow-providers-imap (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_imap-3.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting apache-airflow-providers-smtp (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_smtp-1.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting apache-airflow-providers-sqlite (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_sqlite-3.8.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting pyyaml (from dag-factory)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting iniconfig (from pytest)\n",
      "  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting Mako (from alembic<2.0,>=1.13.1->apache-airflow)\n",
      "  Using cached Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4 (from alembic<2.0,>=1.13.1->apache-airflow)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting flask-appbuilder==4.5.0 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached Flask_AppBuilder-4.5.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting flask-login>=0.6.2 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached Flask_Login-0.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jmespath>=0.7.0 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting apispec<7,>=6.0.0 (from apispec[yaml]<7,>=6.0.0->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached apispec-6.6.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting colorama<1,>=0.3.9 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting click<9,>=8 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting email-validator>=1.0.5 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting Flask-Babel<3,>=1 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached Flask_Babel-2.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting Flask-Limiter<4,>3 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached Flask_Limiter-3.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting Flask-SQLAlchemy<3,>=2.4 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached Flask_SQLAlchemy-2.5.1-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting Flask-JWT-Extended<5.0.0,>=4.0.0 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached Flask_JWT_Extended-4.6.0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting marshmallow<4,>=3.18.0 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting marshmallow-sqlalchemy<0.29.0,>=0.22.0 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached marshmallow_sqlalchemy-0.28.2-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting prison<1.0.0,>=0.2.1 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached prison-0.2.1-py2.py3-none-any.whl.metadata (973 bytes)\n",
      "Collecting sqlalchemy-utils<1,>=0.32.21 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached SQLAlchemy_Utils-0.41.2-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting WTForms<4 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached wtforms-3.1.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting clickclick<21,>=1.2 (from connexion<3.0,>=2.10.0->connexion[flask]<3.0,>=2.10.0->apache-airflow)\n",
      "  Using cached clickclick-20.10.2-py2.py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting inflection<0.6,>=0.3.1 (from connexion<3.0,>=2.10.0->connexion[flask]<3.0,>=2.10.0->apache-airflow)\n",
      "  Using cached inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting werkzeug<3,>=2.0 (from apache-airflow)\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting pytz>2021.1 (from croniter>=2.0.2->apache-airflow)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=39.0.0->apache-airflow)\n",
      "  Using cached cffi-1.17.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.13->apache-airflow)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting cachelib<0.10.0,>=0.9.0 (from flask-caching>=1.5.0->apache-airflow)\n",
      "  Using cached cachelib-0.9.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->apache-airflow)\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->apache-airflow)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->apache-airflow)\n",
      "  Using cached rpds_py-0.20.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting uc-micro-py (from linkify-it-py>=2.0.0->apache-airflow)\n",
      "  Using cached uc_micro_py-1.0.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.1.0->apache-airflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting wirerope>=0.4.7 (from methodtools>=0.4.7->apache-airflow)\n",
      "  Using cached wirerope-0.4.7-py3-none-any.whl\n",
      "Collecting importlib-metadata<=8.0.0,>=6.0 (from opentelemetry-api>=1.15.0->apache-airflow)\n",
      "  Using cached importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tzdata>=2020.1 (from pendulum<4.0,>=2.1.2->apache-airflow)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting time-machine>=2.6.0 (from pendulum<4.0,>=2.1.2->apache-airflow)\n",
      "  Using cached time_machine-2.15.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (21 kB)\n",
      "Collecting docutils (from python-daemon>=3.0.0->apache-airflow)\n",
      "  Using cached docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting setuptools>=62.4.0 (from python-daemon>=3.0.0->apache-airflow)\n",
      "  Using cached setuptools-72.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.3->apache-airflow) (1.16.0)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify>=5.0->apache-airflow)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27.0->apache-airflow)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.27.0->apache-airflow)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27.0->apache-airflow)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.27.0->apache-airflow)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting more-itertools>=9.0.0 (from apache-airflow-providers-common-sql->apache-airflow)\n",
      "  Using cached more_itertools-10.4.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting sqlparse>=0.4.2 (from apache-airflow-providers-common-sql->apache-airflow)\n",
      "  Using cached sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting aiohttp>=3.9.2 (from apache-airflow-providers-http->apache-airflow)\n",
      "  Using cached aiohttp-3.10.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting requests_toolbelt (from apache-airflow-providers-http->apache-airflow)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anyio (from httpx->apache-airflow)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx->apache-airflow)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sniffio (from httpx->apache-airflow)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->apache-airflow)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio<2.0.0,>=1.0.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached grpcio-1.65.4-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk~=1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf<5.0,>=3.19 (from opentelemetry-proto==1.26.0->opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Using cached aiohappyeyeballs-2.3.5-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=39.0.0->apache-airflow)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.15.0->apache-airflow)\n",
      "  Using cached zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting apache-airflow-providers-cncf-kubernetes (from apache-airflow)\n",
      "  Using cached apache_airflow_providers_cncf_kubernetes-8.3.4-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=1.0.5->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting Babel>=2.3 (from Flask-Babel<3,>=1->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting limits>=3.13 (from Flask-Limiter<4,>3->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached limits-3.13.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting ordered-set<5,>4 (from Flask-Limiter<4,>3->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk~=1.26.0->opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp->apache-airflow)\n",
      "  Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting aiofiles>=23.2.0 (from apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting kubernetes<=30.1.0,>=29.0.0 (from apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting kubernetes_asyncio<=30.1.0,>=29.0.0 (from apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached kubernetes_asyncio-30.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes<=30.1.0,>=29.0.0->apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached google_auth-2.33.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes<=30.1.0,>=29.0.0->apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes<=30.1.0,>=29.0.0->apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes<=30.1.0,>=29.0.0->apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting importlib-resources>=1.3 (from limits>=3.13->Flask-Limiter<4,>3->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes<=30.1.0,>=29.0.0->apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached cachetools-5.4.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes<=30.1.0,>=29.0.0->apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes<=30.1.0,>=29.0.0->apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes<=30.1.0,>=29.0.0->apache-airflow-providers-cncf-kubernetes->apache-airflow)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using cached apache_airflow-2.9.3-py3-none-any.whl (13.3 MB)\n",
      "Using cached dag_factory-0.19.0-py2.py3-none-any.whl (16 kB)\n",
      "Using cached pytest-8.3.2-py3-none-any.whl (341 kB)\n",
      "Using cached alembic-1.13.2-py3-none-any.whl (232 kB)\n",
      "Using cached apache_airflow_providers_fab-1.2.2-py3-none-any.whl (79 kB)\n",
      "Using cached Flask_AppBuilder-4.5.0-py3-none-any.whl (2.2 MB)\n",
      "Using cached argcomplete-3.5.0-py3-none-any.whl (43 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Using cached colorlog-4.8.0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached ConfigUpdater-3.2-py2.py3-none-any.whl (34 kB)\n",
      "Using cached connexion-2.14.2-py2.py3-none-any.whl (95 kB)\n",
      "Using cached cron_descriptor-1.4.3-py3-none-any.whl (49 kB)\n",
      "Using cached croniter-3.0.3-py2.py3-none-any.whl (22 kB)\n",
      "Using cached cryptography-43.0.0-cp39-abi3-macosx_10_9_universal2.whl (6.2 MB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached Flask-2.2.5-py3-none-any.whl (101 kB)\n",
      "Using cached Flask_Caching-2.3.0-py3-none-any.whl (28 kB)\n",
      "Using cached flask_session-0.5.0-py3-none-any.whl (7.2 kB)\n",
      "Using cached flask_wtf-1.2.1-py3-none-any.whl (12 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached google_re2-1.1.20240702-1-cp312-cp312-macosx_14_0_arm64.whl (461 kB)\n",
      "Using cached gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
      "Using cached lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached marshmallow_oneofschema-3.1.1-py3-none-any.whl (5.7 kB)\n",
      "Using cached mdit_py_plugins-0.4.1-py3-none-any.whl (54 kB)\n",
      "Using cached opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Using cached pendulum-3.0.0-cp312-cp312-macosx_11_0_arm64.whl (352 kB)\n",
      "Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Using cached PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Using cached python_daemon-3.0.1-py3-none-any.whl (31 kB)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached rich_argparse-1.5.2-py3-none-any.whl (19 kB)\n",
      "Using cached setproctitle-1.3.3-cp312-cp312-macosx_10_9_universal2.whl (16 kB)\n",
      "Using cached SQLAlchemy-1.4.53-cp312-cp312-macosx_10_9_universal2.whl (1.6 MB)\n",
      "Using cached SQLAlchemy_JSONField-1.0.2-py3-none-any.whl (10 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached universal_pathlib-0.2.2-py3-none-any.whl (46 kB)\n",
      "Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Using cached apache_airflow_providers_common_io-1.4.0-py3-none-any.whl (18 kB)\n",
      "Using cached apache_airflow_providers_common_sql-1.15.0-py3-none-any.whl (46 kB)\n",
      "Using cached apache_airflow_providers_ftp-3.10.1-py3-none-any.whl (19 kB)\n",
      "Using cached apache_airflow_providers_http-4.12.0-py3-none-any.whl (27 kB)\n",
      "Using cached apache_airflow_providers_imap-3.6.1-py3-none-any.whl (17 kB)\n",
      "Using cached apache_airflow_providers_smtp-1.7.1-py3-none-any.whl (22 kB)\n",
      "Using cached apache_airflow_providers_sqlite-3.8.2-py3-none-any.whl (13 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
      "Using cached aiohttp-3.10.2-cp312-cp312-macosx_11_0_arm64.whl (387 kB)\n",
      "Using cached cachelib-0.9.0-py3-none-any.whl (15 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached cffi-1.17.0-cp312-cp312-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached clickclick-20.10.2-py2.py3-none-any.whl (7.4 kB)\n",
      "Using cached Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
      "Using cached inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached more_itertools-10.4.0-py3-none-any.whl (60 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.20.0-cp312-cp312-macosx_11_0_arm64.whl (313 kB)\n",
      "Using cached setuptools-72.1.0-py3-none-any.whl (2.3 MB)\n",
      "Using cached sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Using cached time_machine-2.15.0-cp312-cp312-macosx_11_0_arm64.whl (17 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "Using cached Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
      "Using cached aiohappyeyeballs-2.3.5-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached apispec-6.6.1-py3-none-any.whl (30 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Using cached Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\n",
      "Using cached Flask_JWT_Extended-4.6.0-py2.py3-none-any.whl (22 kB)\n",
      "Using cached Flask_Limiter-3.8.0-py3-none-any.whl (28 kB)\n",
      "Using cached Flask_SQLAlchemy-2.5.1-py2.py3-none-any.whl (17 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Using cached grpcio-1.65.4-cp312-cp312-macosx_10_9_universal2.whl (10.4 MB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached marshmallow_sqlalchemy-0.28.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
      "Using cached prison-0.2.1-py2.py3-none-any.whl (5.8 kB)\n",
      "Using cached SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Using cached zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Using cached apache_airflow_providers_cncf_kubernetes-8.3.4-py3-none-any.whl (145 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Using cached kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Using cached kubernetes_asyncio-30.1.0-py3-none-any.whl (1.8 MB)\n",
      "Using cached limits-3.13.0-py3-none-any.whl (45 kB)\n",
      "Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached google_auth-2.33.0-py2.py3-none-any.whl (200 kB)\n",
      "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached cachetools-5.4.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Installing collected packages: unicodecsv, text-unidecode, pytz, py4j, lockfile, cron-descriptor, colorlog, zipp, wrapt, wirerope, websocket-client, urllib3, uc-micro-py, tzdata, typing-extensions, termcolor, tenacity, tabulate, sqlparse, sqlalchemy, sniffio, setuptools, setproctitle, rpds-py, rfc3339-validator, pyyaml, python-slugify, pyspark, pyjwt, pycparser, pyasn1, protobuf, prison, pluggy, pathspec, ordered-set, oauthlib, multidict, more-itertools, mdurl, marshmallow, markupsafe, lazy-object-proxy, jmespath, itsdangerous, iniconfig, inflection, importlib-resources, idna, h11, gunicorn, grpcio, google-re2, fsspec, frozenlist, docutils, dnspython, dill, configupdater, colorama, click, charset-normalizer, certifi, cachetools, cachelib, blinker, Babel, attrs, asgiref, argcomplete, apispec, aiohappyeyeballs, aiofiles, yarl, WTForms, werkzeug, universal-pathlib, time-machine, sqlalchemy-utils, sqlalchemy-jsonfield, rsa, requests, referencing, python-daemon, pytest, pyasn1-modules, opentelemetry-proto, methodtools, marshmallow-sqlalchemy, marshmallow-oneofschema, markdown-it-py, Mako, linkify-it-py, jinja2, importlib-metadata, httpcore, googleapis-common-protos, email-validator, deprecated, croniter, clickclick, cffi, anyio, aiosignal, rich, requests_toolbelt, requests-oauthlib, python-nvd3, pendulum, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, mdit-py-plugins, limits, jsonschema-specifications, httpx, google-auth, flask, cryptography, alembic, aiohttp, rich-argparse, opentelemetry-semantic-conventions, kubernetes_asyncio, kubernetes, jsonschema, flask-wtf, Flask-SQLAlchemy, flask-session, flask-login, Flask-Limiter, Flask-JWT-Extended, flask-caching, Flask-Babel, opentelemetry-sdk, flask-appbuilder, connexion, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, apache-airflow-providers-smtp, apache-airflow-providers-imap, apache-airflow-providers-http, apache-airflow-providers-ftp, apache-airflow-providers-fab, apache-airflow-providers-common-sql, apache-airflow-providers-common-io, apache-airflow-providers-sqlite, apache-airflow, apache-airflow-providers-cncf-kubernetes, dag-factory\n",
      "Successfully installed Babel-2.16.0 Flask-Babel-2.0.0 Flask-JWT-Extended-4.6.0 Flask-Limiter-3.8.0 Flask-SQLAlchemy-2.5.1 Mako-1.3.5 WTForms-3.1.2 aiofiles-24.1.0 aiohappyeyeballs-2.3.5 aiohttp-3.10.2 aiosignal-1.3.1 alembic-1.13.2 anyio-4.4.0 apache-airflow-2.9.3 apache-airflow-providers-cncf-kubernetes-8.3.4 apache-airflow-providers-common-io-1.4.0 apache-airflow-providers-common-sql-1.15.0 apache-airflow-providers-fab-1.2.2 apache-airflow-providers-ftp-3.10.1 apache-airflow-providers-http-4.12.0 apache-airflow-providers-imap-3.6.1 apache-airflow-providers-smtp-1.7.1 apache-airflow-providers-sqlite-3.8.2 apispec-6.6.1 argcomplete-3.5.0 asgiref-3.8.1 attrs-24.2.0 blinker-1.8.2 cachelib-0.9.0 cachetools-5.4.0 certifi-2024.7.4 cffi-1.17.0 charset-normalizer-3.3.2 click-8.1.7 clickclick-20.10.2 colorama-0.4.6 colorlog-4.8.0 configupdater-3.2 connexion-2.14.2 cron-descriptor-1.4.3 croniter-3.0.3 cryptography-43.0.0 dag-factory-0.19.0 deprecated-1.2.14 dill-0.3.8 dnspython-2.6.1 docutils-0.21.2 email-validator-2.2.0 flask-2.2.5 flask-appbuilder-4.5.0 flask-caching-2.3.0 flask-login-0.6.3 flask-session-0.5.0 flask-wtf-1.2.1 frozenlist-1.4.1 fsspec-2024.6.1 google-auth-2.33.0 google-re2-1.1.20240702 googleapis-common-protos-1.63.2 grpcio-1.65.4 gunicorn-22.0.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 importlib-metadata-8.0.0 importlib-resources-6.4.0 inflection-0.5.1 iniconfig-2.0.0 itsdangerous-2.2.0 jinja2-3.1.4 jmespath-1.0.1 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 kubernetes-30.1.0 kubernetes_asyncio-30.1.0 lazy-object-proxy-1.10.0 limits-3.13.0 linkify-it-py-2.0.3 lockfile-0.12.2 markdown-it-py-3.0.0 markupsafe-2.1.5 marshmallow-3.21.3 marshmallow-oneofschema-3.1.1 marshmallow-sqlalchemy-0.28.2 mdit-py-plugins-0.4.1 mdurl-0.1.2 methodtools-0.4.7 more-itertools-10.4.0 multidict-6.0.5 oauthlib-3.2.2 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 ordered-set-4.1.0 pathspec-0.12.1 pendulum-3.0.0 pluggy-1.5.0 prison-0.2.1 protobuf-4.25.4 py4j-0.10.9.7 pyasn1-0.6.0 pyasn1-modules-0.4.0 pycparser-2.22 pyjwt-2.9.0 pyspark-3.5.1 pytest-8.3.2 python-daemon-3.0.1 python-nvd3-0.16.0 python-slugify-8.0.4 pytz-2024.1 pyyaml-6.0.2 referencing-0.35.1 requests-2.32.3 requests-oauthlib-2.0.0 requests_toolbelt-1.0.0 rfc3339-validator-0.1.4 rich-13.7.1 rich-argparse-1.5.2 rpds-py-0.20.0 rsa-4.9 setproctitle-1.3.3 setuptools-72.1.0 sniffio-1.3.1 sqlalchemy-1.4.53 sqlalchemy-jsonfield-1.0.2 sqlalchemy-utils-0.41.2 sqlparse-0.5.1 tabulate-0.9.0 tenacity-9.0.0 termcolor-2.4.0 text-unidecode-1.3 time-machine-2.15.0 typing-extensions-4.12.2 tzdata-2024.1 uc-micro-py-1.0.3 unicodecsv-0.14.1 universal-pathlib-0.2.2 urllib3-2.2.2 websocket-client-1.8.0 werkzeug-2.2.3 wirerope-0.4.7 wrapt-1.16.0 yarl-1.9.4 zipp-3.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install pyspark apache-airflow dag-factory pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/09 12:55:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|John|\n",
      "|  2| Doe|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL Demo\").getOrCreate()\n",
    "df = spark.createDataFrame([(1, 'John'), (2, 'Doe')], ['id', 'name'])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file generated\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate a CSV file of 50 lines with the following columns: \n",
    "id, name, age, city, country, position, salary\n",
    "\"\"\"\n",
    "import random\n",
    "import csv\n",
    "\n",
    "first_names = ['John', 'Jane', 'Doe', 'Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace']\n",
    "last_names = ['Smith', 'Johnson', 'Williams', 'Jones', 'Brown', 'Davis', 'Miller', 'Wilson', 'Moore', 'Taylor']\n",
    "cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose']\n",
    "countries = ['USA', 'Canada', 'Mexico', 'Brazil', 'Argentina', 'Chile', 'Peru', 'Colombia', 'Venezuela', 'Ecuador']\n",
    "positions = ['Software Engineer', 'Data Scientist', 'Product Manager', 'Sales Manager', 'Marketing Manager', 'HR Manager', 'Accountant', 'Lawyer', 'Doctor', 'Nurse']\n",
    "salaries = [50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000]\n",
    "\n",
    "# Make the data folder\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "with open('data/input.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id', 'name', 'age', 'city', 'country', 'position', 'salary'])\n",
    "    for i in range(50):\n",
    "        id = i + 1\n",
    "        name = random.choice(first_names) + ' ' + random.choice(last_names)\n",
    "        age = random.randint(20, 60)\n",
    "        city = random.choice(cities)\n",
    "        country = random.choice(countries)\n",
    "        position = random.choice(positions)\n",
    "        salary = random.choice(salaries)\n",
    "        writer.writerow([id, name, age, city, country, position, salary])\n",
    "\n",
    "print('CSV file generated')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create src directory\n",
    "if not os.path.exists('src'):\n",
    "    os.makedirs('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will generate a file with the same code. Run that file to generate output data. Make sure this works before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/etl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/etl.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def extract_data(spark, file_path):\n",
    "    # Extract data from a CSV file\n",
    "    return spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "def transform_data(df):\n",
    "    # Perform a simple transformation: filter and select specific columns\n",
    "    return df.filter(col(\"age\") > 35).select(\"name\", \"age\", \"city\")\n",
    "\n",
    "def load_data(df, output_path):\n",
    "    # Load transformed data into a new CSV file\n",
    "    df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"Simple ETL\").getOrCreate()\n",
    "\n",
    "    # Define file paths\n",
    "    input_file = \"data/input.csv\"\n",
    "    output_file = \"data/output\"\n",
    "\n",
    "    # Execute ETL process\n",
    "    data = extract_data(spark, input_file)\n",
    "    transformed_data = transform_data(data)\n",
    "    load_data(transformed_data, output_file)\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 13:16:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Run file\n",
    "%run src/etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dags directory\n",
    "if not os.path.exists('dags'):\n",
    "    os.makedirs('dags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dags/etl_dag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dags/etl_dag.py\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.dirname(__file__) + '/../src'))\n",
    "\n",
    "# Import ETL functions\n",
    "from etl import extract_data, transform_data, load_data\n",
    "\n",
    "def etl_pipeline():\n",
    "    print(\"Setting up pipeline...\")\n",
    "    # Set up the Spark session\n",
    "    spark = SparkSession.builder.appName(\"Airflow ETL\").getOrCreate()\n",
    "\n",
    "    # Define file paths\n",
    "    input_file = \"data/input.csv\"\n",
    "    output_file = \"data/output\"\n",
    "\n",
    "    # Execute ETL process\n",
    "    data = extract_data(spark, input_file)\n",
    "    transformed_data = transform_data(data)\n",
    "    load_data(transformed_data, output_file)\n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "    print(\"Finito!\")\n",
    "\n",
    "# Define default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(\n",
    "    'simple_etl_dag_from_script',\n",
    "    default_args=default_args,\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    # Define the ETL task\n",
    "    etl_task = PythonOperator(\n",
    "        task_id='run_etl',\n",
    "        python_callable=etl_pipeline,\n",
    "    )\n",
    "\n",
    "    etl_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the DAG file to Airflow home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: Copy etl_dag.py to the dags folder (command below)\n",
    "\n",
    "#!cp dags/etl_dag.py ~/airflow/dags\n",
    "\n",
    "# option 2: Update airflow configuration to include the this project's dags folder\n",
    "# within the config file, replace the dags_folder line with the following:\n",
    "# dags_folder = /path/to/this/project/dags\n",
    "\n",
    "#!open ~/airflow/airflow.cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Airflow database, webserver, and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/airflow/cli/commands/\u001b[0m\u001b[1;33mdb_command.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m48\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: `db init` is deprecated.  Use `db migrate` instead to migrate the db and/or airflow connections create-default-connections to create the default connections\u001b[0m\n",
      "DB: sqlite:////Users/jamison.ducey/airflow/airflow.db\n",
      "[\u001b[34m2024-08-09T13:03:01.391-0600\u001b[0m] {\u001b[34mmigration.py:\u001b[0m215} INFO\u001b[0m - Context impl \u001b[01mSQLiteImpl\u001b[22m.\u001b[0m\n",
      "[\u001b[34m2024-08-09T13:03:01.391-0600\u001b[0m] {\u001b[34mmigration.py:\u001b[0m218} INFO\u001b[0m - Will assume \u001b[01mnon-transactional\u001b[22m DDL.\u001b[0m\n",
      "[\u001b[34m2024-08-09T13:03:01.447-0600\u001b[0m] {\u001b[34mmigration.py:\u001b[0m215} INFO\u001b[0m - Context impl \u001b[01mSQLiteImpl\u001b[22m.\u001b[0m\n",
      "[\u001b[34m2024-08-09T13:03:01.448-0600\u001b[0m] {\u001b[34mmigration.py:\u001b[0m218} INFO\u001b[0m - Will assume \u001b[01mnon-transactional\u001b[22m DDL.\u001b[0m\n",
      "[\u001b[34m2024-08-09T13:03:01.448-0600\u001b[0m] {\u001b[34mdb.py:\u001b[0m1625} INFO\u001b[0m - Creating tables\u001b[0m\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "WARNI [unusual_prefix_513eeade7e200caf78e1e8d4f2a9fa297ae49150_example_python_operator] The virtalenv_python example task requires virtualenv, please install it.\n",
      "WARNI [unusual_prefix_50103b34285308bcc84e1c52283dc9ba2be4308e_tutorial_taskflow_api_virtualenv] The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.\n",
      "WARNI [unusual_prefix_013732f8d09a07a2224936102044040a1c90a07c_example_python_decorator] The virtalenv_python example task requires virtualenv, please install it.\n",
      "WARNI [unusual_prefix_64a040751d7d7fa72d328db58c1d5e35114c1103_workday] Could not import pandas. Holidays will not be considered.\n",
      "WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.\n",
      "Initialization done\n",
      "\u001b[1;33m/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/flask_limiter/\u001b[0m\u001b[1;33mextension.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m333\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: \u001b[0m\u001b[4;33mhttps://flask-limiter.readthedocs.io#configuring-a-storage-backend\u001b[0m\u001b[33m for documentation about configuring the storage backend.\u001b[0m\n",
      "Password:Traceback (most recent call last):\n",
      "  File \"/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/bin/airflow\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/airflow/__main__.py\", line 58, in main\n",
      "    args.func(args)\n",
      "  File \"/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/airflow/cli/cli_config.py\", line 49, in command\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/airflow/utils/cli.py\", line 114, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py\", line 55, in wrapped_function\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/airflow/providers/fab/auth_manager/cli_commands/user_command.py\", line 74, in users_create\n",
      "    password = _create_password(args)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/airflow/providers/fab/auth_manager/cli_commands/user_command.py\", line 120, in _create_password\n",
      "    password = getpass.getpass(\"Password:\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!airflow db init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m/Users/jamison.ducey/Desktop/etl_with_copilot/.venv/lib/python3.12/site-packages/flask_limiter/\u001b[0m\u001b[1;33mextension.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m333\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: \u001b[0m\u001b[4;33mhttps://flask-limiter.readthedocs.io#configuring-a-storage-backend\u001b[0m\u001b[33m for documentation about configuring the storage backend.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admin already exist in the db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['airflow', 'users', 'create', '--username', 'admin', '--firstname', 'John', '--lastname', 'Doe', '--role', 'Admin', '--email', 'admin@example.org', '--password', 'admin'], returncode=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "\n",
    "# Prompt for the password\n",
    "password = getpass.getpass(\"Enter the password for the new Airflow user: \")\n",
    "\n",
    "# Construct the command\n",
    "command = [\n",
    "    \"airflow\", \"users\", \"create\",\n",
    "    \"--username\", \"admin\",\n",
    "    \"--firstname\", \"John\",\n",
    "    \"--lastname\", \"Doe\",\n",
    "    \"--role\", \"Admin\",\n",
    "    \"--email\", \"admin@example.org\",\n",
    "    \"--password\", password\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airflow webserver started\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Start the Airflow webserver\n",
    "webserver_process = subprocess.Popen([\"airflow\", \"webserver\", \"--port\", \"8080\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "print(\"Airflow webserver started\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airflow scheduler started\n"
     ]
    }
   ],
   "source": [
    "# Start the Airflow scheduler\n",
    "scheduler_process = subprocess.Popen([\"airflow\", \"scheduler\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "print(\"Airflow scheduler started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the Airflow UI\n",
    "!open http://localhost:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airflow webserver and scheduler stopped\n"
     ]
    }
   ],
   "source": [
    "# Closing the webserver and scheduler processes when you're finished. Run the cells above again to reopen.\n",
    "webserver_process.terminate()\n",
    "scheduler_process.terminate()\n",
    "\n",
    "print(\"Airflow webserver and scheduler stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a YAML for dag-factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dags/etl_cfg.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dags/etl_cfg.yml\n",
    "\n",
    "simple_etl_dag_auto:\n",
    "  default_args:\n",
    "    owner: 'airflow'\n",
    "    start_date: '2024-01-01'\n",
    "    retries: 1\n",
    "  schedule_interval: '@daily'\n",
    "  catchup: False\n",
    "  tasks:\n",
    "    etl_task:\n",
    "      operator: airflow.operators.python.PythonOperator\n",
    "      python_callable_name: 'etl_pipeline'\n",
    "      python_callable_file: /Users/jamison.ducey/etl_copilot_demo/dags/etl_dag.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write DAG generator script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dags/generate_dags.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dags/generate_dags.py\n",
    "\n",
    "from airflow import DAG  ## by default, this is needed for the dagbag to parse this file\n",
    "import dagfactory\n",
    "from pathlib import Path\n",
    "\n",
    "config_file = Path.cwd() / \"dags/etl_cfg.yml\"\n",
    "print(f\"config_file: {config_file}\")\n",
    "dag_factory = dagfactory.DagFactory(config_file)\n",
    "\n",
    "dag_factory.clean_dags(globals())\n",
    "dag_factory.generate_dags(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test those manually by running the dag-factory script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_file: /Users/jamison.ducey/Desktop/etl_with_copilot/dags/etl_cfg.yml\n"
     ]
    }
   ],
   "source": [
    "# Run the script\n",
    "!python dags/generate_dags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tests directory\n",
    "if not os.path.exists('tests'):\n",
    "    os.makedirs('tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/test_etl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_etl.py\n",
    "\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add src directory to the system path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.dirname(__file__) + '/../src'))\n",
    "\n",
    "from etl import extract_data, transform_data, load_data\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def spark():\n",
    "    # Setup: Create a Spark session\n",
    "    spark = SparkSession.builder.appName(\"Test ETL\").getOrCreate()\n",
    "    yield spark\n",
    "    # Teardown: Stop the Spark session\n",
    "    spark.stop()\n",
    "\n",
    "def test_extract_data(spark):\n",
    "    # Create a test CSV file\n",
    "    test_file = \"/tmp/test_input.csv\"\n",
    "    with open(test_file, \"w\") as f:\n",
    "        f.write(\"name,age,city\\nJohn Doe,45,New York\\nJane Doe,30,Los Angeles\\nAlice,40,Chicago\\n\")\n",
    "    \n",
    "    # Test the extract_data function\n",
    "    df = extract_data(spark, test_file)\n",
    "    \n",
    "    # Define the expected output\n",
    "    expected_data = [(\"John Doe\", 45, \"New York\"),\n",
    "                     (\"Jane Doe\", 30, \"Los Angeles\"),\n",
    "                     (\"Alice\", 40, \"Chicago\")]\n",
    "    expected_df = spark.createDataFrame(expected_data, [\"name\", \"age\", \"city\"])\n",
    "    \n",
    "    # Check if the result matches the expected output\n",
    "    assert df.collect() == expected_df.collect()\n",
    "\n",
    "    # Clean up test file\n",
    "    os.remove(test_file)\n",
    "\n",
    "def test_transform_data(spark):\n",
    "    # Create a DataFrame with test data\n",
    "    test_data = [(\"John Doe\", 45, \"New York\"),\n",
    "                 (\"Jane Doe\", 30, \"Los Angeles\"),\n",
    "                 (\"Alice\", 40, \"Chicago\")]\n",
    "    df = spark.createDataFrame(test_data, [\"name\", \"age\", \"city\"])\n",
    "    \n",
    "    # Apply the transform_data function\n",
    "    result_df = transform_data(df)\n",
    "    \n",
    "    # Define the expected output\n",
    "    expected_data = [(\"John Doe\", 45, \"New York\"),\n",
    "                     (\"Alice\", 40, \"Chicago\")]\n",
    "    expected_df = spark.createDataFrame(expected_data, [\"name\", \"age\", \"city\"])\n",
    "    \n",
    "    # Check if the result matches the expected output\n",
    "    assert result_df.collect() == expected_df.collect()\n",
    "\n",
    "def test_load_data(spark):\n",
    "    # Create a DataFrame with test data\n",
    "    test_data = [(\"John Doe\", 45, \"New York\"),\n",
    "                 (\"Alice\", 40, \"Chicago\")]\n",
    "    df = spark.createDataFrame(test_data, [\"name\", \"age\", \"city\"])\n",
    "    \n",
    "    # Define the output path\n",
    "    output_path = \"/tmp/test_output\"\n",
    "    \n",
    "    # Apply the load_data function\n",
    "    load_data(df, output_path)\n",
    "    \n",
    "    # Read the output data back\n",
    "    result_df = spark.read.csv(output_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Define the expected output\n",
    "    expected_df = df\n",
    "    \n",
    "    # Check if the result matches the expected output\n",
    "    assert result_df.collect() == expected_df.collect()\n",
    "    \n",
    "    # Clean up output directory\n",
    "    os.system(f\"rm -r {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tests manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.4, pytest-8.3.2, pluggy-1.5.0\n",
      "rootdir: /Users/jamison.ducey/Desktop/etl_with_copilot\n",
      "plugins: time-machine-2.15.0, anyio-4.4.0\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_etl.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 6.81s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the tests\n",
    "!pytest tests/test_etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a Github Actions workflow for CI/CD testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make .github/workflows directory\n",
    "if not os.path.exists('.github/workflows'):\n",
    "    os.makedirs('.github/workflows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .github/workflows/ci.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .github/workflows/ci.yml\n",
    "\n",
    "name: CI\n",
    "\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    services:\n",
    "      spark:\n",
    "        image: apache/spark:latest\n",
    "        ports:\n",
    "          - 4040:4040\n",
    "        options: --memory=4g --cpus=2\n",
    "\n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v2\n",
    "\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: '3.x'\n",
    "\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "\n",
    "    - name: Run PySpark tests\n",
    "      run: |\n",
    "        pytest tests/test_etl.py\n",
    "\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: test  # This ensures the deployment only runs if the test job succeeds\n",
    "\n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v2\n",
    "\n",
    "    - name: Deploy application\n",
    "      run: |\n",
    "        # Add your deployment commands here\n",
    "        echo \"Deploying application...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate requirements.txt\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write('pyspark\\n')\n",
    "    f.write('apache-airflow\\n')\n",
    "    f.write('dag-factory\\n')\n",
    "    f.write('pytest\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make .gitignore file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .gitignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gitignore\n",
    "\n",
    "# Ignore Python bytecode files\n",
    ".pytest_cache\n",
    ".venv\n",
    "**/__pycache__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push to repository and watch the jobs run in Github Actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished, congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
